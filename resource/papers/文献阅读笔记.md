# 一、SwarmChain: Collaborative LLM Inference for UAV Swarm Control

## 研究背景与动机

### **主要背景**

- **UAV群应用广泛**：无人机群在物流、监控等领域有广泛应用，但在恶劣环境下缺乏地面指挥中心时，控制管理困难
- **传统方法局限**：现有机器人依赖预定义规则，在动态环境中适应性差
- **LLM技术机遇**：大语言模型可以理解自然语言指令并生成执行代码

### **核心问题**

1. **资源约束**：无人机计算能力和内存有限，无法直接部署大型LLM
2. **LLM局限**：存在幻觉问题，缺乏无人机控制的专业背景知识
3. **实时性要求**：需要快速响应和执行任务指令

### **研究动机**

**目标**：让资源受限的无人机群能够通过自然语言指令实现智能协作控制

**解决思路**：

- 通过多设备协作推理分担LLM计算负载
- 设计资源感知的任务调度算法
- 整合背景知识和模板系统提高LLM可靠性

## 核心贡献

### **1. CoLLM协作推理框架**

- **功能**：专门针对资源受限设备集群设计的协作推理框架
- **创新点**：通过张量并行将LLM分割部署到多个设备上，实现分布式协作推理
- **解决问题**：让计算能力有限的无人机能够协作运行大型LLM

### **2. Also自适应负载调度算法**

- **功能**：资源感知的自适应负载调度方案
- **创新点**：动态感知各节点的CPU负载、内存使用、计算时间、网络延迟等资源状态，优化任务分配
- **解决问题**：应对UAV群的动态网络、资源不均衡和设备异构性挑战

### **3. SwarmChain完整系统**

- **功能**：LLM与UAV群控制的端到端集成系统

- 创新点

  **指令模板化**：动态提示模板系统

  **背景知识嵌入**：向量数据库存储和检索

  **历史信息管理**：维护任务执行上下文

  **输出解析**：将LLM生成内容转换为具体UAV控制指令

- **解决问题**：减少LLM幻觉，确保UAV能够准确执行任务

## 系统模型

## **整体架构**

SwarmChain系统由三个核心模块组成，形成完整的协作推理控制链：

```
指令输入 → CoLLM推理 → 控制输出
    ↑        ↑         ↓
SwarmChain ← Also调度 → UAV执行
```

------

### **1. CoLLM协作推理框架**

### **基本原理**

- 利用**张量并行**将LLM计算任务分解到多个设备
- 基于Transformer的多头注意力机制天然并行性

### **工作流程**

1. **计算最优设备数量**：根据模型大小和设备能力确定参与设备数
2. **自适应权重分割**：将模型权重均匀分布到各设备，避免内存过载
3. **建立连接池和缓冲区**：设备间数据通信和结果传输
4. 执行推理任务
   - 输入经过嵌入层生成向量
   - 分发到多设备并行执行Transformer块
   - 通过通信机制聚合中间结果
   - 应用Softmax得到最终结果

### **核心技术**

- **KV缓存机制**：缓存之前的计算结果，减少冗余计算
- **RoPE位置编码**：有效捕获位置信息
- **MPI通信**：协调设备间的计算过程

------

### **2. Also自适应负载调度算法**

### **问题建模**

- **目标**：最小化总推理延迟（计算延迟 + 通信延迟 + 同步延迟）
- 约束条件
  1. 资源阈值约束（计算能力、内存、剩余电量）
  2. 任务完整性约束（所有张量分区必须被处理）
  3. 负载均衡约束（设备间负载差异在允许范围内）
  4. 资源容量约束（不超过设备内存和能耗限制）

### **算法流程**

```
1. 节点筛选：筛选满足资源阈值的可用节点
2. 任务排序：按计算复杂度降序排列任务
3. 节点排序：按计算能力降序排列节点  
4. 贪心分配：为每个任务选择当前负载最轻的合适节点
5. 负载均衡调整：迁移任务以平衡设备负载
```

### **时间复杂度**：O(KN)，K为任务数，N为节点数

------

### **3. SwarmChain控制模块**

### **核心组件**

#### **动态提示模板**

- **任务分类器**：识别用户输入的任务类型
- **模板库**：存储不同任务类型的提示模板
- **动态填充**：根据任务类型和环境信息动态生成提示

```
示例："为无人机规划路径以在{环境}中导航，约束条件为{限制}"
```

#### **上下文跟踪器**

- 持续记录当前任务、环境状态、历史交互数据
- 支持多步推理和实时动态任务的上下文管理

#### **向量数据库**

- 存储任务信息、无人机API、历史数据的嵌入向量
- 支持高效的相似性搜索和信息检索
- 实现智能的历史状态管理和任务连续性

#### **输出解析器**

- 从LLM生成内容中提取关键代码
- 转换为具体的无人机控制指令格式
- 过滤描述性文本，发送可执行命令给UAV群

### **工作流程**

```
1. 自然语言指令 → 提示模板模块处理
2. 向量数据库检索 → 背景知识注入
3. CoLLM协作推理 → 生成原始输出
4. 输出解析器处理 → 转换为控制指令
5. 直接接口UAV API → 执行任务
```

## 

# 二、Large Language Model-Based Task Offloading and Resource Allocation for Digital Twin Edge Computing Networks-基于大语言模型的数字孪生边缘计算网络任务卸载与资源分配任务卸载与资源分配

## 研究背景与动机

###  背景

-  物联网的增长极大扩展了车辆应用，导致车辆产生大量计算任务，而车辆自身的计算资源有限，难以独立处理所有任务
-  环境动态性和车辆移动性造成任务卸载量的不确定性，导致车辆间竞争服务器计算资源

###  动机

-  传统深度强化学习方法训练复杂，容易陷入局部最优
-  大语言模型(LLM)推理效率高，只需设计提示词
-  缺乏针对数字孪生车辆网络中多任务队列稳定性的LLM解决方案
-  目标：用LLM替代传统方法，同时优化系统服务质量和能耗

## 核心贡献

### **建立了新的系统**模型

- 提出了通用的数字孪生边缘计算网络场景
- 分析了车辆在单个时隙内生成多种异构任务导致的服务器队列积压问题
- 研究了队列任务对系统平均QoS和总能耗的影响

### **优化方法创新**

- 应用Lyapunov优化将涉及队列稳定性的长期约束转换为可处理的短期决策问题
- 将原始的长期稳定性问题重构为短期决策问题

### **提出基于**LLM的解决方案

- 采用基于大语言模型的上下文学习方法替代传统的多智能体强化学习(MARL)框架
- 通过设计提示词让LLM输出解决方案，结合MARL提供的案例集
- 这是首次将LLM应用于数字孪生车辆网络资源分配问题

## 系统模型

###  网络架构

 论文构建了一个双层数字孪生车辆边缘计算网络：

 **物理层**：

- N辆车辆在道路上行驶
- 1个配备服务器的基站
- 车辆在基站覆盖范围内移动

 **云数字孪生层**：

- 物理层实体在云端的数字副本
- 通过孪生内通信进行信息同步

## 算法结构

### 李雅普诺夫优化

**1. 李雅普诺夫函数**：
$$
L(Q(t)) = \frac{1}{2}\sum_{k=1}^K (q_k(t))^2
$$
**2. 条件李雅普诺夫偏移**：
$$
\Delta Q(t) \triangleq E\{L(Q(t+1)) - L(Q(t))|Q(t)\}
$$
**3. 偏移上界**：
$$
\Delta Q(t) \leq B - \sum_{k=1}^K q_k(t)\left(\frac{f_E\sum_{n=1}^N \alpha_{n,k}^t}{c_k} - Z_k(t)\right)
$$
**4. 重构问题P2**：
$$
\min_{\omega_{n,k}^t,\alpha_{n,k}^t} \beta\sum_{k=1}^K q_k(t)\left(Z_k(t) - \frac{f_E\sum_{n=1}^N \alpha_{n,k}^t}{c_k}\right) + (-U^{sys}(t) + E^{sys}(t))
$$

### MARL框架

**1. 状态空间**：
$$
s_n(t) = \{\Gamma_n(t), l_n(t), v_n, g_{n,b}^t\}
$$
**2. 动作空间**：
$$
a_n(t) = \{\omega_{n,k}^t, \alpha_{n,k}^t\}
$$
**3. 奖励函数**：
$$
r_n(t) = U_n^{ave} - \eta\left(f_E - \left(\sum_{n=1}^N\sum_{k=1}^K \alpha_{n,k}^t f_E + \Delta f_{n,k}^{est}\right)\right)
$$

### LLM上下文学习

**上下文学习公式**：
$$
D_{task} \times \epsilon_{example}^t \times s^t \times LLM \Rightarrow A^t
$$

# 三、LLM Meets the Sky: Heuristic Multi-Agent
Reinforcement Learning for Secure Heterogeneous
UAV Networks

## 1.研究背景与动机

### 背景

- UAV运动与通信的强耦合导致优化问题复杂
- 异构UAV间缺乏有效经验共享，样本效率低
- 传统优化方法计算复杂度高且容易陷入局部最优

### 动机

现有研究多假设UAV能力均匀或忽略能耗-安全权衡，异构UAV网络的安全问题仍是空白领域。本文旨在提出新的优化框架，利用大语言模型的推理能力指导异构UAV的协作决策，实现安全性与能效的有效平衡

## 2.核心贡献

### 2.1.问题建模创新

#### **首次研究异构UAV网络的物理层安全问题**

- 建立了现实的多UAV辅助安全通信系统模型，考虑UAV间不同的覆盖范围和服务能力
- 将安全性与能效权衡问题建模为多目标优化（MOO），同时优化保密率最大化和推进能耗最小化

### 2.2.分层优化框架

#### **提出S2DC+LLM-HeMARL的分层解决方案**

- **内层**：S2DC算法处理固定UAV位置下的保密预编码优化，使用半定松弛、精确惩罚和差凸规划技术
- **外层**：LLM-HeMARL处理异构UAV协作轨迹优化，有效解耦了运动与通信变量的复杂耦合

### 2.3. LLM驱动的启发式多智能体强化学习

#### **创新性的三阶段LLM应用方法**

- **阶段1**：LLM专家策略收集和提示调优，生成异构性感知的专家决策
- **阶段2**：通过离线强化学习将LLM策略蒸馏为快速策略，使用保守Q学习避免分布偏移
- **阶段3**：在线策略适应，进一步优化以适应未见环境状态

#### **解决了关键技术挑战**：

- 避免LLM高推理延迟问题（通过预计算而非实时推理）
- 减少异构UAV间的盲目探索
- 提供异构性感知的协作决策指导

## 3.系统模型

### 3.1 网络组成

- **异构UAV集合**: $\mathcal{K} = \{1, 2, ..., N_K\} $，具有不同载荷和计算能力
- **地面终端(GT)**: $\mathcal{I} = \{1, 2, ..., N_I\} $，静态分布的合法用户
- **窃听者(Eve)**: $\mathcal{E} = \{1, 2, ..., N_E\} $，恶意监听节点
- **服务区域**: $D \times D $ 米的矩形区域
- **时间离散化**: $\mathcal{T} = \{1, 2, ..., N_T\} $ 个时隙，每个时隙长度为 $\Delta t $

### 3.2 UAV异构性特征

每个UAV k具有以下特性：

- **覆盖范围**: $C_k^r $ (米)
- **服务容量**: $N_k^s $ (最大同时服务用户数)
- **天线数量**: $M $ 根
- **飞行高度**: $H_{UAV} $ (固定)
- **位置**: $u_k(t) = [x_k(t), y_k(t), H_{UAV}] $

### 3.3 RSMA传输机制

采用Rate-Splitting Multiple Access（RSMA）技术：

每个消息 $W_{k,i} $ 分解为：

- **公共部分**: $W_{k,i}^c $ → 公共流 $s_k^c $
- **私有部分**: $W_{k,i}^p $ → 私有流 $s_{k,i}^p $

### 3.3 SINR计算

**合法用户的SINR**:
$$
γ_i^c = (S_{k,i}^I |h_{k,i}^H p_k^c|²) / (S_{k,i}^I ∑|h_{k,i}^H p_{k,i'}^p|² + I_i^{in} + σ_i²)
$$

$$
γ_i^p = (S_{k,i}^I |h_{k,i}^H p_{k,i}^p|²) / (S_{k,i}^I ∑|h_{k,i}^H p_{k,i'}^p|² + I_i^{in} + σ_i²)
$$



**窃听者的SINR**:
$$
γ_{e,i}^c = (A_{k,e}^E |h_{k,e}^H p_k^c|²) / (A_{k,e}^E ∑|h_{k,e}^H p_{k,i'}^p|² + I_{e,i}^{in} + σ_e²)
$$

$$
γ_{e,i}^p = (A_{k,e}^E |h_{k,e}^H p_{k,i}^p|²) / (A_{k,e}^E (|h_{k,e}^H p_k^c|² + ∑|h_{k,e}^H p_{k,i'}^p|²) + I_{e,i}^{in} + σ_e²)
$$

### 3.4 空对地信道

信道系数包含大尺度衰落和小尺度衰落：
$$
h_{k,x}(t) = \sqrt{(10^{-ℓ_{k,x}(t)/10})} ĥ_{k,x}(t)
$$

### 3.5 推进功率消耗

旋翼UAV的推进功率：
$$
P_k(v_k(t)) = (1/2)d_0 ρ_a s_sol A v_k(t)³ + P_0(1 + 3v_k(t)²/v_tip²) + P_1\sqrt{(1 + v_k(t)⁴/4v_0⁴ - v_k(t)²/2v_0²)}
$$

## 4.算法结构

### 4.1层次化架构

#### 内层：S2DC 安全预编码

**目标函数：**
$$
\max_{P} F_1 = \min_{k,i,e} (R_i(t) - R_{e,i}(t))
$$
**约束条件：**
$$
\text{tr}(P_k(t)P_k^H(t)) \leq P_{\max}, \quad R_i^c(t) \geq R_{e,i}^c(t)
$$
**技术路线：**

- 半定松弛(SDR)处理rank-1约束
- 差凸优化分解非凸目标函数：$F_1 = F_{1,1} + F_{1,2} - (F_{1,3} + F_{1,4}) $
- 精确惩罚方法处理非凸约束

#### 外层：LLM-HeMARL 轨迹优化

**MDP建模：**

- 状态空间：$s_t^k = \{u_k(t) - u_l(t), u_k(t) - u_i, u_k(t) - u_e\} $
- 动作空间：$a_t^k = \{v_k(t), \omega_k(t)\} $
- 奖励函数：$r^k = (w_{sr} \cdot r_{sr} + w_{ec} \cdot r_{ec}) \times \eta_{loc} \times \eta_{col} $

### 4.2三阶段学习框架

#### 阶段1：LLM专家策略收集

通过精心设计的提示工程，让LLM生成专家轨迹数据集：
$$
\mathcal{D}_{LLM} = \{(s_t, a_t, r_t, s_{t+1}) | a_t \sim \pi_{LLM}(a_t|s_t)\}
$$

#### 阶段2：保守策略蒸馏

使用CQL(Conservative Q-Learning)将LLM智慧转移到神经网络：

**CQL损失函数：**
$$
\mathcal{L}_{dis}(\phi_i, \mathcal{D}_{LLM}) = \mathcal{L}(\phi_i, \mathcal{D}_{LLM}) + \beta \mathbb{E}\left[\log\sum_{a_t} \exp(Q(s_t,a_t)) - \mathbb{E}_{\pi_{LLM}}[Q(s_t,a_t)]\right]
$$
**ISAC更新规则：**

- 评论家更新：$\phi_{i,dis}^{(\iota+1)} \leftarrow \arg\min_{\phi_{i,dis}} \mathcal{L}_{dis}(\phi_{i,dis}, \mathcal{D}_{LLM}) $
- 演员更新：$\psi_{dis}^{(\iota+1)} \leftarrow \arg\min_{\psi_{dis}} \mathcal{L}_{dis}(\psi_{dis}, \mathcal{D}_{LLM}) $
- 软更新：$\hat{\phi}_i = \tau\phi_i + (1-\tau)\hat{\phi}_i $

#### 阶段3：在线策略适应

基于蒸馏策略进行环境适应：
$$
\psi_{on}^{(0)} = \psi_{dis}, \quad \phi_{i,on}^{(0)} = \phi_{i,dis}
$$
**在线更新：**
$$
\phi_{i,on}^{(\iota+1)} \leftarrow \arg\min_{\phi_{i,on}} \mathcal{L}_{on}(\phi_{i,on}, \mathcal{D}_{on})
$$

# 四、Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and
Communication in Integrated Terrestrial and Non-Terrestrial Networks

## 1.研究背景与动机

- 多无人机系统的协调控制比单机控制复杂得多，需要处理更高维的状态空间和无人机间的同步问题

**应用场景** 论文考虑的是一个3D空中高速公路场景，多架无人机需要在多车道空域中飞行，类似于地面交通系统但更加复杂。

## 2.核心贡献

**1. 架构创新** 首次提出基于大语言模型的分层协作控制框架，将HAPS元控制器与无人机边缘控制器有机结合，实现全局网络管理与局部实时决策的协同优化。

**2. 问题建模突破** 将多无人机运动控制与通信决策统一建模为联合优化问题，突破了现有研究将两者分离处理的局限，首次在3D空中高速公路场景下同时考虑加速/减速、变道和基站选择。

**3. 技术方法创新**

- 利用LLM的预训练知识和上下文学习能力，无需针对特定任务进行大量重训练
- 通过指令跟踪机制直接集成安全约束（最小间距、最大切换率等）
- 设计了分层决策机制：HAPS层处理战略级资源分配，无人机层处理战术级实时控制

## 3.系统模型

### 3.1网络架构

系统考虑一个集成地面与非地面网络的空中高速公路场景，包含：

- **$N_{UAV} $架无人机** ：沿高速公路车道飞行
- **基站集合$\mathcal{B} = \{b_1, \ldots, b_{N_R}, b_{HAPS}\} $**：包括地面基站和HAPS
- **连接模式**：无人机可连接HAPS或地面基站

# 4.算法结构

**双层LLM框架**：

- **上层**：HAPS元控制器（全局网络协调）
- **下层**：无人机边缘控制器（实时运动与通信决策）
- **交互方式**：上层发布高级指令，下层执行细粒度控制

# 五、Exploring Multi-Agent Dynamics for Generative AI and Large Language Models in Mobile Edge Networks

### 在移动边缘网络中的潜在应用

1. **边缘-云生成系统** - 在边缘、端和云服务器上部署不同智能体进行协作内容创建
2. **异构无人设备网络** - 为不同无人设备配备专门的GenAI-LLM智能体
3. **近用户内容缓存** - 学习用户习惯和偏好，优化内容缓存
4. **智能交通系统** - 为无人机和无人车配备处理响应、管理和监控功能的智能体
5. **语义通信** - 通过生成上下文相关响应改善语义通信

## 未来拓展方向

1. **实时优化** - 开发能够根据网络状态动态调整的智能体系统
2. **跨模态融合** - 结合文本、图像、音频的多模态LLM应用
3. **联邦学习集成** - 在保护隐私的前提下实现智能体间知识共享
4. **边缘推理优化** - 专门为资源受限的边缘设备优化的轻量级LLM



# 六、Accuracy-Aware MLLM Task Offloading and Resource Allocation in UAV-Assisted Satellite Edge Computing

## 1.背景与动机

**技术背景**

- MLLM计算需求巨大(数十亿参数)，云计算部署存在延迟、带宽、隐私问题
- 传统边缘计算在偏远地区覆盖受限
- 卫星边缘计算(SEC)结合LEO卫星和UAV提供解决方案

**核心挑战**

1. 资源约束：卫星和UAV计算资源有限，难以满足MLLM需求
2. 网络动态性：LEO卫星轨道运动导致拓扑频繁变化
3. 混合优化：离散卸载决策与连续资源分配的联合优化

## 2.核心贡献

### 1. 首个MLLM-SEC联合优化框架

**创新点**：这是第一个专门针对UAV辅助卫星边缘计算网络中多模态大语言模型推理的综合优化框架。

**技术内容**：

- 构建三层SEC网络架构：IoTD-UAV-LEO卫星
- UAV部署较小MLLM（精度Am），LEO卫星部署大型MLLM（精度An）
- 考虑LEO卫星轨道运动的动态覆盖模型

### 2.混合整数非线性规划(MINLP)问题建模

### 3. 动作解耦软演员-评论家(AD-SAC)算法

## 3.系统模型

# 七、EdgeShard: Efficient LLM Inference via
Collaborative Edge Computing

## 1.背景与动机

**大语言模型的挑战：**

- LLM模型参数量巨大（如Llama2-7B需要28GB内存，70B需要280GB），超出了大多数边缘设备的承载能力
- 传统部署方式依赖云端，存在高延迟、高带宽成本和隐私泄露风险
- 对于实时应用（如机器人控制、导航）来说，云端推理的响应延迟无法满足需求

## 2.核心贡献

### 1. 首创性框架贡献

**首个协作边缘计算LLM推理框架**

- 这是第一个在协作边缘计算环境中部署LLM的工作
- 突破了传统"云端部署"或"边缘-云协作"的二元模式
- 提出了多设备横向协作的新范式

### 2. 理论建模与优化算法

**联合设备选择与模型分片问题**

- 将LLM部署形式化为数学优化问题
- 同时考虑设备选择和模型分片两个维度（现有工作通常只考虑其中一个）
- 设计了两个动态规划算法：
  - 延迟优化算法：时间复杂度O(N×M²)
  - 吞吐量优化算法：时间复杂度O(N²×2^M×M²)

### 3. 系统架构设计

**三阶段框架**

1. **离线Profile阶段**：系统化地收集设备性能和网络特征
2. **任务调度优化**：基于动态规划的智能分片策略
3. **协作推理执行**：支持顺序推理和流水线并行两种模式

**流水线优化策略**

- EdgeShard-Bubbles：传统流水线方法
- EdgeShard-No-Bubbles：创新的无气泡优化，提升资源利用率

# 八、MARL framework

## 1.QMIX 算法核心流程讲解

### 核心思想

QMIX 解决多智能体的**信用分配问题**：当团队获得奖励时，如何知道每个智能体的贡献？

**解决方案**：学习一个智能混合函数

```
团队Q值 = QMIX(个体Q值1, 个体Q值2, 个体Q值3, 全局状态)
```

------

### 网络架构

**4个网络**：

1. **主智能体网络** - 所有智能体共享，输出个体Q值
2. **主QMIX网络** - 将个体Q值混合成团队Q值
3. **目标智能体网络** - 提供稳定的目标个体Q值
4. **目标QMIX网络** - 提供稳定的目标团队Q值

------

### 训练数据

**个体数据**（每个智能体独有）：

- 观察：敌人位置、队友状态、自身信息
- 动作：攻击、移动等选择
- 可用动作：当前可执行的动作掩码

**共享数据**（团队共享）：

- 全局状态：所有单位的完整信息
- 团队奖励：环境给出的团队总奖励
- 结束标志：游戏是否结束

------

### 训练流程

### 1. 前向传播

```
智能体观察 → 共享智能体网络 → 个体Q值 
[Q1, Q2, Q3] + 全局状态 → QMIX网络 → 团队Q值
```

### 2. 目标计算

```
下一状态 → 目标网络 → 目标团队Q值
TD目标 = 团队奖励 + 0.99 × 目标团队Q值
```

### 3. 损失计算

```
损失 = (当前团队Q值 - TD目标)²
```

### 4. 梯度传播

```
团队损失 → QMIX网络梯度 → 个体Q值梯度 → 智能体网络梯度
```

## 2.QTRAN 算法核心流程讲解

### 核心思想

QTRAN 突破 QMIX 的**单调性限制**，学习更一般的价值分解函数，能处理非单调的复杂协作场景。

**解决方案**：通过变换学习任意复杂的价值分解

```
团队Q值 = f(个体Q值1, 个体Q值2, 个体Q值3) + V(全局状态)
```

------

### 网络架构

**6个网络**：

1. **主智能体网络** - 所有智能体共享，输出个体Q值
2. **主联合Q网络** - 计算联合动作的Q值
3. **主状态价值网络** - 计算状态价值V(s)
4. **目标智能体网络** - 提供稳定的目标个体Q值
5. **目标联合Q网络** - 提供稳定的目标联合Q值
6. **目标状态价值网络** - 提供稳定的目标状态价值

------

### 训练数据

**与QMIX相同**：

- **个体数据**：观察、动作、可用动作
- **共享数据**：全局状态、团队奖励、结束标志

**额外需求**：

- **动作编码**：将所有智能体动作编码为联合动作
- **反事实动作**：计算"如果某智能体选择其他动作"的组合

------

### 训练流程

### 1. 前向传播

```
智能体观察 → 共享网络 → 个体Q值 [Q1, Q2, Q3]
全局状态 + 联合动作 → 联合Q网络 → Q_joint
全局状态 → 状态价值网络 → V(s)
```

### 2. 三个损失函数

#### TD损失（主要学习目标）

```
TD_loss = (Q_joint - TD目标)²
TD目标 = 团队奖励 + 0.99 × 目标Q_joint
```

#### Opt损失（最优动作一致性）

```
确保：argmax(Q_joint) = (argmax(Q1), argmax(Q2), argmax(Q3))
Opt_loss = (个体Q值之和 - Q_joint + V(s))²
```

#### Nopt损失（次优动作惩罚）

```
惩罚非最优动作组合：
Nopt_loss = max(0, 个体Q值之和 - Q_joint + V(s))²
```

### 3. 总损失

```
总损失 = TD_loss + λ₁ × Opt_loss + λ₂ × Nopt_loss
```

# 九、Cost-Efficient Task Offloading in Mobile Edge
Computing With Layered Unmanned
Aerial Vehicles

## 1.背景与动机

- 随着移动用户（MU）对高效节能服务的需求激增，移动边缘计算（MEC）应运而生，它允许MU将计算密集型任务卸载到网络边缘，以减少骨干网络拥塞，并实现更节能、更具成本效益的通信 。
- 近年来，无人机（UAV）辅助的MEC成为一种新的模式，因为无人机具有高移动性、轻便性，并且能够在不受地理限制的条件下为MU提供不间断的服务 。然而，现有的研究很少全面考虑无人机辅助MEC网络中的计算和缓存问题 。此外，系统中的任务计算不均衡和异构性使得缓存数据和卸载位置的选择变得复杂 。
- 本研究的动机是解决这些挑战，并在此基础上构建一个分层无人机辅助的混合云-边缘系统，将地面云中心整合到多无人机MEC系统中 

## 2.核心贡献

### 1. 系统模型与问题建模的创新

**分层UAV混合云边系统架构**：

- 设计了一个三层架构：地面移动用户(MUs)、边缘UAVs(EUAVs)、以及与云端连接的链接UAV(LUAV)
- EUAVs既作为边缘服务器提供计算服务，又作为中继节点将任务转发到云端
- 同时考虑了缓存和计算两种服务，提高了资源利用效率

**静态与动态应用支持**：

- 静态应用：必须完全在本地执行或完全卸载
- 动态应用：支持部分卸载，可以任意比例分割执行
- 这种建模更贴近实际应用场景的多样性

### 2. 优化算法设计

**AGSP算法**：论文设计了一个自适应遗传模拟退火粒子群优化算法，融合了三种算法的优势：

- **粒子群优化(PSO)**：提供基础的群体智能搜索机制
- **遗传算法(GA)**：通过交叉和变异操作增强种群多样性
- **模拟退火(SA)**：使用Metropolis接受准则避免陷入局部最优

**关键技术特点**：

- 非线性动态自适应惯性权重：根据种群进化差异动态调整探索与开发的平衡
- 自适应更新策略：根据粒子适应度比值采用不同的速度和位置更新公式
- 时间复杂度为O(ĝXM)，其中ĝ是迭代次数，X是粒子数，M是用户数

### 3. 多目标联合优化

**综合优化目标**：

- 同时优化系统总能耗和任务延迟
- 使用加权成本函数平衡两个冲突目标
- 权重系数根据用户服务质量(QoS)偏好动态确定

**多维决策变量**：

- 传输功率分配(P)
- 计算资源分配(F)
- 二进制变量(D)：缓存决策、卸载决策
- EUAV坐标(U)
- 卸载比例(A)

## 3.系统模型

## 系统架构

**三层结构设计**：

1. **云层**：中心云服务器，具有强大的计算能力
2. **边缘层**：多个边缘UAVs (EUAVs)，集合表示为 $\mathcal{K} = \{1, 2, \ldots, K\} $
3. **用户层**：地面移动用户(MUs)，集合表示为 $\mathcal{M} = \{1, 2, \ldots, M\} $

**EUAV坐标约束**：
$$
\check{\alpha} \leq \alpha_k \leq \hat{\alpha}
$$
其中 $u_k = [\alpha_k, \beta_k] $ 表示EUAV $k $ 的二维坐标。

## 通信模型

**信道增益建模**：
$$
h_{m,k} = \Omega_{m,k}\sqrt{\xi_{m,k}}
$$
**瑞利衰落系数**：
$$
\Omega_{m,k} = \sqrt{\frac{k_{m,k}}{k_{m,k} + 1}}\bar{\Omega}_{m,k} + \sqrt{\frac{1}{k_{m,k} + 1}}\tilde{\Omega}_{m,k}
$$
**瑞利因子**：
$$
k_{m,k} = k_0 \cdot e^{\tilde{k}\theta_{m,k}}
$$
其中 $\theta_{m,k} = \arcsin(\tilde{h}/d_{m,k}) $，$d_{m,k} = \sqrt{\tilde{h}^2 + \|u_m - u_k\|^2} $

**大尺度路径损耗**：
$$
\xi_{m,k} = A d_{m,k}^{-\gamma_{m,k}}
$$
**传输速率**：

下行链路（EUAV到MU）：
$$
r_{k,m}^{\downarrow} = B_{m,k} \log_2\left(1 + \frac{p_{k,m}|h_{m,k}|^2}{\sigma^2 + \sum_{k'=1, k'\neq k}^K p_{k',m}|h_{m,k'}|^2}\right)
$$
上行链路（MU到EUAV）：
$$
r_{m,k}^{\uparrow} = B_{m,k} \log_2\left(1 + \frac{q_{m,k}|h_{m,k}|^2}{\sigma^2 + \sum_{m'=1, m'\neq m}^M q_{m',k}|h_{m',k}|^2}\right)
$$
**功率约束**：
$$
0 \leq p_{k,m} \leq \hat{p}_k, \quad 0 \leq q_{m,k} \leq \hat{q}_m
$$

## 缓存与计算模型

**缓存约束**：
$$
\sum_{m=1}^M \lambda_{m,k} C_m \leq C_k
$$
其中 $\lambda_{m,k} \in \{0,1\} $ 为二进制缓存变量。

**卸载决策约束**：
$$
\sum_{k=1}^K (\mu_{m,k}^E + \mu_{m,k}^L) \leq 1
$$
其中 $\mu_{m,k}^E, \mu_{m,k}^L \in \{0,1\} $

**静态与动态应用**： $$a_{m,k} = \begin{cases} 1, & \omega_m = 1 \ [0,1], & \omega_m = 0 \end{cases}$$ 其中 $\omega_m \in {0,1}$ 表示应用类型。

**计算能力约束**：
$$
0 \leq f_m \leq \hat{f}_m
$$

## 延迟模型

**总延迟公式**：
$$
T_m = \sum_{k=1}^K T_{m,k}^c + \max\left\{T_m^l, \sum_{k=1}^K (T_{m,k}^E + T_{m,k}^L + T_{m,k}^t)\right\}
$$
**各组成部分**：

缓存文件传输延迟：
$$
T_{m,k}^c = \frac{C_m}{r_{k,m}^{\downarrow}} + (1-\lambda_{m,k})\frac{C_m}{r_{0,k}^{\downarrow}}
$$
本地计算延迟：
$$
T_m^l = \frac{(1-a_{m,k})\zeta_m}{f_m}
$$
边缘计算延迟：
$$
T_{m,k}^E = \mu_{m,k}^E a_{m,k} \frac{\zeta_m}{f_{m,k}^E}
$$
云计算延迟：
$$
T_{m,k}^L = \mu_{m,k}^L a_{m,k} \frac{\zeta_m}{f_{m,k}^L}
$$
传输延迟：
$$
T_{m,k}^t = \mu_{m,k}^E \left(\frac{a_{m,k}D_m}{r_{m,k}^{\uparrow}} + \frac{O_m}{r_{k,m}^{\downarrow}}\right) + \mu_{m,k}^L \left(\frac{a_{m,k}D_m}{r_{m,k}^{\uparrow}} + \frac{O_m}{r_{k,m}^{\downarrow}} + \frac{a_{m,k}D_m}{r_{k,0}^{\uparrow}} + \frac{O_m}{r_{0,k}^{\downarrow}}\right)
$$

## 能耗模型

**总能耗公式**：
$$
\varepsilon_m = \sum_{k=1}^K \varepsilon_{m,k}^c + \varepsilon_m^l + \sum_{k=1}^K (\varepsilon_{m,k}^E + \varepsilon_{m,k}^L + \varepsilon_{m,k}^t)
$$
**各组成部分**：

缓存传输能耗：
$$
\varepsilon_{m,k}^c = \frac{C_m}{r_{k,m}^{\downarrow}} p_{k,m} + (1-\lambda_{m,k})\frac{C_m}{r_{0,k}^{\downarrow}} p_{0,k}
$$
本地计算能耗：
$$
\varepsilon_m^l = \kappa_m (f_m)^3 T_m^l
$$
边缘计算能耗：
$$
\varepsilon_{m,k}^E = T_{m,k}^E \kappa_k \left(s_1 + s_2 \left(\frac{f_{m,k}^E}{\bar{e}}\right)\right)
$$
云计算能耗：
$$
\varepsilon_{m,k}^L = T_{m,k}^L \kappa_0 \left(s_3 + s_4 \left(\frac{f_{m,k}^L}{\bar{e}}\right)\right)
$$
传输能耗：
$$
\varepsilon_{m,k}^t = \mu_{m,k}^E \left(\frac{a_{m,k}D_m}{r_{m,k}^{\uparrow}} q_{m,k} + \frac{O_m}{r_{k,m}^{\downarrow}} p_{k,m}\right) + \mu_{m,k}^L \left(\frac{a_{m,k}D_m}{r_{m,k}^{\uparrow}} q_{m,k} + \frac{O_m}{r_{k,m}^{\downarrow}} p_{k,m} + \frac{a_{m,k}D_m}{r_{k,0}^{\uparrow}} q_{k,0} + \frac{O_m}{r_{0,k}^{\downarrow}} p_{0,k}\right)
$$

## 优化问题建模

**目标函数**：
$$
\min_{P,F,D,U,A} \Gamma = \sum_{m=1}^M (\rho_1^m \varepsilon_m + \rho_2^m T_m)
$$
**权重系数**：
$$
\rho_i^m = \frac{1}{2}\left(\frac{\iota_{i,1}^m}{\iota_{1,1}^m + \iota_{1,2}^m} + \frac{\iota_{i,2}^m}{\iota_{1,2}^m + \iota_{2,2}^m}\right)
$$
**约束条件**：
$$
T_m \leq \hat{T}_m, \quad \varepsilon_m \leq \hat{\varepsilon}_m
$$
**惩罚函数法**：
$$
\min_{P,F,D,U,A} \tilde{\Gamma} = \infty \mathcal{N} \mathcal{U} + \Gamma
$$
其中惩罚项：
$$
\mathcal{U} = \sum_{\ell=1}^{N_{\neq}} (\max\{0, -\vartheta_{\ell}(x)\})^{\gamma_1} + \sum_{\ell=1}^{N_=} |\varphi_{\ell}(x)|^{\gamma_2}
$$

## 4.算法结构

**AGSP = PSO + GA + SA + 自适应机制**

核心思想是将粒子群优化作为基础框架，通过遗传算法增强多样性，用模拟退火避免局部最优，并引入自适应权重平衡探索与开发。

## 核心组件结构

### 1. 粒子群优化基础框架

**粒子表示**：

- 每个粒子 $x_i $ 包含所有决策变量：$P, F, D, U, A $
- 粒子维度：$N = 11M + 1 $（存储所有变量和适应度值）
- 种群规模：$X = 100 $ 个粒子

**基本更新公式**：
$$
v_i = w^g v_i + c_1 \vartheta_1 (\varrho_i - x_i) + c_2 \vartheta_2 (\varrho^* - x_i)
$$

### 2. 自适应惯性权重机制

**核心创新 - 非线性动态权重**：
$$
w^g = \hat{w} + (\check{w} - \hat{w}) \frac{1}{1 + \exp\left[-10b\left(\frac{2g}{\vartheta^g \cdot \hat{g}} - 1\right)\right]}
$$
**进化差异度量**： $$\vartheta^g = \begin{cases} 1, & g = 1 \ \frac{\text{StdFit}(g)}{\text{StdFit}(g-1)}, & g > 1 \end{cases}$$

这个设计使算法在早期探索时使用较高权重，后期开发时使用较低权重。

### 3. 遗传算法操作

**交叉操作**：
$$
y_i = \frac{c_1 \cdot \vartheta_1 \cdot \varrho_i + c_2 \cdot \vartheta_2 \cdot \varrho^*}{c_1 \cdot \vartheta_1 + c_2 \cdot \vartheta_2}
$$
**变异操作**：

- 将 $\varrho_i $ 和 $\varrho^* $ 编码为二进制字符串
- 以概率 $\varpi $ 对每个比特进行变异
- 通过单点交叉生成后代 $z_i $

**贪心选择**： $$x_i^g = \begin{cases} y_i, & \text{if } \tilde{\Gamma}(y_i) \leq \tilde{\Gamma}(z_i) \ z_i, & \text{else} \end{cases}$$

### 4. 自适应更新策略

**适应度比率**：
$$
j_i = \frac{\exp(\tilde{\Gamma}(x_i^g))}{\exp\left(\frac{1}{X} \sum_{i=1}^X \tilde{\Gamma}(x_i^g)\right)}
$$
**分支更新机制**：

当 $j_i \geq \delta $（分散分布）：
$$
v_i = w^g \cdot v_i + c_1 \cdot \vartheta_1 \left(\frac{\varrho_i + \varrho^*}{2} - x_i\right) + c_2 \vartheta_2 \left(\frac{\varrho_i - \varrho^*}{2} - x_i\right)
$$
当 $j_i < \delta $（集中分布）：
$$
v_i = w^g v_i + c_1 \vartheta_1 (\bar{\varrho}^g - x_i) + c_2 \vartheta_2 (\varrho^* - x_i)
$$
其中 $\bar{\varrho}^g = \frac{1}{N} \sum_{i=1}^N x_{i,n}^g $ 是种群平均位置。

### 5. 模拟退火接受机制

**Metropolis准则**：
$$
\text{接受} x_i^{g+1} \text{ if } e^{\frac{\tilde{\Gamma}(x_i^g) - \tilde{\Gamma}(x_i^{g+1})}{t^g}} > \vartheta_3
$$
**温度调度**：
$$
t^g = \hat{t} \cdot t^g, \quad t^1 = 10^8, \quad \hat{t} = 0.95
$$


# 十、TD3-based trajectory optimization for energy consumption minimization in
UAV-assisted MEC system

## 1.背景与动机

**题**：传统MEC系统依赖地面基站，在灾害等紧急情况下容易失效，无法提供关键通信服务。

**解决方案**：利用UAV作为移动基站，在基础设施损坏时提供应急MEC服务。

**关键挑战**：

- UAV电池有限，需要优化飞行轨迹减少能耗
- 用户位置随机，需要智能任务卸载策略
- 多目标优化问题（能耗、延迟、覆盖率）非凸难解

## 2.核心贡献

**技术创新**：

- **PDPSO算法**：优化任务卸载决策
- **TD3算法**：优化UAV三维飞行轨迹
- **组合优势**：PDPSO探索能力强，TD3在连续动作空间表现优异

**主要贡献**：

1. 首次将UAV飞行能耗纳入系统总能耗模型
2. 创新性地结合PDPSO和TD3算法
3. 提出基于数字孪生的实际部署方案

## 3.系统模型

**时间划分**：
$$
\text{时隙数量} = \frac{T}{\chi}
$$

### 2. 通信模型

**信道增益**：
$$
h_k(t) = \frac{\beta_0}{H^2 + \|S_{uav}(t) - S_k(t)\|^2}
$$
**数据传输速率**：
$$
r_k(t) = \frac{B_u}{K} \log_2\left(1 + \frac{P_{user}h_k(t)}{\sigma_0^2}\right)
$$
**最小速率要求**：
$$
\kappa = \frac{L_k(t)}{\chi}
$$
**通信链路有效性**： $$I_k(t) = \begin{cases} 1, & \text{if } r_k(t) > \kappa \ 0, & \text{otherwise} \end{cases}$$

**传输延迟**：
$$
T_k^{Tra}(t) = \frac{\alpha_k(t)L_k(t)\varphi_k(t)}{r_k(t)}
$$
**传输能耗**：
$$
E_k^{Tra}(t) = P_{uav}T_k^{Tra}(t) = \frac{P_{uav}\alpha_k(t)L_k(t)\varphi_k(t)}{r_k(t)}
$$

### 3. 计算模型

**本地计算延迟**：
$$
T_{k,loc}^{Com}(t) = \frac{(1-\varphi_k(t))L_k(t)C_{user}}{f_{user}}
$$
**本地计算能耗**：
$$
E_{k,loc}^{Com}(t) = K_{user}(f_{user})^3T_{k,loc}^{Com}(t) = K_{user}(1-\varphi_k(t))L_k(t)C_{user}(f_{user})^2
$$
**UEs部分计算延迟**：
$$
T_{k,par}^{Com}(t) = \frac{(1-\alpha_k(t))L_k(t)\varphi_k(t)C_{user}}{f_{user}}
$$
**UEs部分计算能耗**：
$$
E_{k,par}^{Com}(t) = K_{user}(1-\alpha_k(t))L_k(t)\varphi_k(t)C_{user}(f_{user})^2
$$
**UAV计算延迟**：
$$
T_{k,uav}^{Com}(t) = \frac{\varphi_k(t)\alpha_k(t)L_k(t)C_{uav}}{f_{uav}}
$$
**UAV计算能耗**：
$$
E_{k,uav}^{Com}(t) = K_{uav}\varphi_k(t)\alpha_k(t)L_k(t)C_{uav}(f_{uav})^2
$$
**最优卸载比例**：
$$
\alpha_k(t) = \frac{C_{user}f_{uav}r_k(t)}{(C_{uav}f_{user} + C_{user}f_{uav})r_k(t) + f_{user}f_{uav}}
$$

### 4. UAV飞行能耗模型

**功率模型**：
$$
P_e(V) = P_0\left(1 + \frac{3V^2}{U_{tip}^2}\right) + P_i\left(\sqrt{1 + \frac{V^4}{4V_0^2}} - \frac{V^2}{2V_0^2}\right)^{\frac{1}{2}} + \frac{1}{2}d_0\rho sAV^3
$$
**时隙能耗**：
$$
E_{uav}(t) = P_e(V_t) \times \chi
$$
**总飞行能耗**：
$$
E_{total} = \sum_{t=1}^T E_{uav}(t)
$$

### 5. 系统总体模型

**部分卸载总延迟**：
$$
T_{par}(t) = \max(T_{k,par}^{Com}(t), T_k^{Tra}(t) + T_{k,uav}^{Com}(t))
$$
**部分卸载总能耗**：
$$
E_{par}(t) = E_k^{Tra}(t) + E_{k,par}^{Com}(t) + E_{k,uav}^{Com}(t)
$$
**系统总延迟**：
$$
T(t) = \sum_{k=1}^K[(1-\varphi_k(t))T_{k,loc}^{Com}(t) + \varphi_k(t)T_{par}(t)]
$$
**系统总能耗**：
$$
E(t) = \sum_{k=1}^K[(1-\varphi_k(t))E_{k,loc}^{Com}(t) + \varphi_k(t)E_{par}(t)]
$$

### 6. 优化问题

**目标函数**：
$$
\min_{S_{uav}(t)} \omega\left[\sum_{t=1}^T E(t)\right] + (1-\omega)\left[\sum_{t=1}^T T(t)\right] + \vartheta E_{total}
$$
**约束条件**：

- $C_1: S_{uav}(t), S_k(t) \in \{X_{size}, Y_{size}, H\} $
- $C_2: \varphi_k(t) \in \{0,1\} $
- $C_3: \min[T_{par}(t)] $
- $C_4: S_{uav}^{start} \to S_{uav}^{end} $
- $C_5: I_k(t) = 1 $
- $C_6: V_t \leq V_{max} $

## 4.算法结构

## PDPSO-TD3算法结构

### 1. 整体架构设计

**双层优化框架**：

- **上层**：PDPSO算法优化任务卸载策略
- **下层**：TD3算法优化UAV轨迹规划
- **协同机制**：两算法通过共享状态和奖励信息实现协同优化

### 2. PDPSO算法组件

**核心改进机制**：

**动态惯性权重调整**： $$\omega_i(t+1) = \begin{cases} \omega_i(t)\left(e^{\frac{1}{D(t+1)+1}} - 1 + 1\right), & D(t+1) \geq D(t) \ \omega_i(t)\left(e^{\frac{1}{D(t+1)+1}} - 1\right), & D(t+1) < D(t) \end{cases}$$

**种群多样性计算**：
$$
D(t+1) = \sqrt{\frac{1}{K-1}\sum_{i=1}^K\left(d_i^{Ave}(t) - d_i^{Min}(t)\right)^2}
$$
**粒子更新规则**：
$$
v_i(t+1) = \omega(t)v_i(t) + c_1rand_1[pbest - x_i(t)] + c_2rand_2[gbest - x_i(t)]
$$
**卸载决策映射**：
$$
Sig(v_i(t)) = \frac{1}{1 + e^{-v_i(t)}}
$$

### 3. TD3算法组件

**双Critic网络架构**：

- **策略网络**：$\mu_n $（主）+ $\mu'_n $（目标）
- **价值网络**：$\theta_n^1, \theta_n^2 $（主）+ $\theta_n^{1'}, \theta_n^{2'} $（目标）

**核心改进机制**：

**双Critic目标计算**：

y_i(r_i, s_{i+1}, \gamma) = r_i + \gamma \min_{i=1,2} Q_{\theta_i'_n}(s_{i+1}, \mu'(s_{i+1}|\theta_{\mu'}))

**延迟策略更新**：

- 每个时间步更新Critic网络
- 每$d $步更新Actor网络

**目标策略噪声**：
$$
a'(s_{i+1}) = clip(\mu'(s_{i+1}|\theta_{\mu'}) + clip(\varepsilon, -c, c), a_{low}, a_{high})
$$
**网络更新规则**：
$$
\nabla_{\mu_n}J(\mu_n) = \frac{1}{N}\sum_i[\nabla_{\mu_n}\mu(s_i;\theta_\mu)\nabla_a Q_{\theta^1}(s,a)|_{s=s_i, a=\mu(s_i;\theta_\mu)}]
$$

### 4. 状态-动作-奖励设计

**状态空间**：
$$
State = \{S_k(t), S_{uav}(t), L_k(t), \forall S \in \{X_{size}, Y_{size}, H\}\}
$$
**动作空间**：
$$
Action = \{o_t, v_t\}
$$
其中$o_t \in [0, 360°] $，$v_t \in [0, V_{max}] $

**奖励函数**：
$$
R_{step} = \min_{S_{uav}(t)} \omega\left[\sum_{t=1}^T E(t)\right] + (1-\omega)\left[\sum_{t=1}^T T(t)\right] + \vartheta E_{total}
$$
**信息流动路径**：

```
状态s_t → TD3Actor → 飞行动作a_t
           ↓
    PDPSO接收a_t → 内部I次优化 → 最优卸载策略φ_optimal
           ↓
    环境执行(a_t, φ_optimal) → 奖励r_t, 新状态s_next
           ↓
    TD3Critic学习 → 更新价值函数 → 改进Actor策略
```
